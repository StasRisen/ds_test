{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIvhwE4zxX0s",
    "outputId": "dfa3b3dd-38a1-4a67-879f-46376640ef14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\stani\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: gensim in c:\\users\\stani\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: bokeh in c:\\users\\stani\\anaconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: umap-learn in c:\\users\\stani\\anaconda3\\lib\\site-packages (0.5.3)\n",
      "Requirement already satisfied: click in c:\\users\\stani\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\stani\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\stani\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from gensim) (5.1.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: contourpy>=1 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from bokeh) (1.0.6)\n",
      "Requirement already satisfied: pillow>=7.1.0 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from bokeh) (9.0.1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from bokeh) (6.0)\n",
      "Requirement already satisfied: Jinja2>=2.9 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from bokeh) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from bokeh) (4.1.1)\n",
      "Requirement already satisfied: packaging>=16.8 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from bokeh) (21.3)\n",
      "Requirement already satisfied: tornado>=5.1 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from bokeh) (6.1)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from bokeh) (1.4.2)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from bokeh) (2022.9.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from umap-learn) (1.0.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from umap-learn) (0.5.8)\n",
      "Requirement already satisfied: numba>=0.49 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from umap-learn) (0.55.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from Jinja2>=2.9->bokeh) (2.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\stani\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn) (61.2.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn) (0.38.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from packaging>=16.8->bokeh) (3.0.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from pandas>=1.2->bokeh) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from pandas>=1.2->bokeh) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.2->bokeh) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\stani\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22->umap-learn) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\stani\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "#Установка нужных пакетов\n",
    "!pip install --upgrade nltk gensim bokeh umap-learn\n",
    "\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import umap\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=267be092c84e2c4abecb430ee2a3aa603bbbc7380707991db36b6d1fe6a2de67\n",
      "  Stored in directory: c:\\users\\stani\\appdata\\local\\pip\\cache\\wheels\\04\\5f\\3e\\46cc37c5d698415694d83f607f833f83f0149e49b3af9d0f38\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hF9WPCtfxZR9",
    "outputId": "0d73ae54-58c4-4d44-bcc7-8d35c800f08b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved under quora.txt\n"
     ]
    }
   ],
   "source": [
    "# выгружаем датасет:\n",
    "!python -m wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1  ./quora.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "MaFpN9pvxtNg",
    "outputId": "f8255b0e-bdfc-47cf-fa91-7f9075b78d14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What TV shows or books help you read people's body language?\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(open(\"./quora.txt\", encoding=\"utf-8\"))\n",
    "data[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HvXRbOKGx0l_",
    "outputId": "f265a2aa-fc44-4763-bc87-fe05ee0a7203"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'TV', 'shows', 'or', 'books', 'help', 'you', 'read', 'people', \"'\", 's', 'body', 'language', '?']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(data[50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovkxi_QOySCl"
   },
   "source": [
    "#Задание 1: Перевести все слова в нижний регистр (NLTK) из data и добавьте как лист токенов в листе data_tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EK7uvHi6zeWY"
   },
   "outputs": [],
   "source": [
    "data_tok = []\n",
    "for i in range(len(data)): \n",
    "  data_tok.append(tokenizer.tokenize(data[i].lower()))\n",
    "\n",
    "\n",
    "\n",
    "#checking\n",
    "\n",
    "assert all(isinstance(row, (list, tuple)) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
    "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
    "assert all(map(lambda l: not is_latin(l) or l.islower(), map(' '.join, data_tok))), \"please make sure to lowercase the data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtKeoLCYzY4j"
   },
   "source": [
    "###Задание 2: Подсчитайте топ10 самых популярных лем в рамках data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "a_BxzSv9yR0w"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Stani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Stani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "dict_words = {}\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_words = {}\n",
    "for list_key in range(len(data_tok)):\n",
    "  for key in data_tok[list_key]:\n",
    "    if lemmatizer.lemmatize(key) in dict_words.keys():\n",
    "      dict_words[lemmatizer.lemmatize(key)] += 1\n",
    "    else:\n",
    "      dict_words.update({lemmatizer.lemmatize(key): 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_lemm = pd.DataFrame(dict_words.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>?</td>\n",
       "      <td>552413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>the</td>\n",
       "      <td>252068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>what</td>\n",
       "      <td>214798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>is</td>\n",
       "      <td>185392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>a</td>\n",
       "      <td>172513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i</td>\n",
       "      <td>149873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>to</td>\n",
       "      <td>141788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>in</td>\n",
       "      <td>139813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>how</td>\n",
       "      <td>135687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>of</td>\n",
       "      <td>112001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1\n",
       "17     ?  552413\n",
       "30   the  252068\n",
       "18  what  214798\n",
       "10    is  185392\n",
       "24     a  172513\n",
       "1      i  149873\n",
       "22    to  141788\n",
       "37    in  139813\n",
       "60   how  135687\n",
       "49    of  112001"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lemm.sort_values(by=1, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1SM3sn1zf1b"
   },
   "source": [
    "###Задание 3: Подсчитайте количество разных слов до и после лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Q88BIteDzpWR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80303\n"
     ]
    }
   ],
   "source": [
    "#Количество разных слов после лемматизации\n",
    "print(len(df_lemm)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for list_ in data_tok:\n",
    "    for value in list_:\n",
    "        words.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87819"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Количество разных слов до лемматизации\n",
    "len(df1[0].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxKa8yUUzqNN"
   },
   "source": [
    "###Задание 4: Подсчитайте количество разных слов до и после стемминга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "dict_words = {}\n",
    "for list_key in range(len(data_tok)):\n",
    "  for key in data_tok[list_key]:\n",
    "    if stemmer.stem(key) in dict_words.keys():\n",
    "      dict_words[stemmer.stem(key)] += 1\n",
    "    else:\n",
    "      dict_words.update({stemmer.stem(key): 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67026\n"
     ]
    }
   ],
   "source": [
    "df_stem = pd.DataFrame(dict_words.items())\n",
    "#Количество разных слов после стемминга\n",
    "print(len(df_stem)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87819"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for list_ in data_tok:\n",
    "    for value in list_:\n",
    "        words.append(value)\n",
    "        \n",
    "df1 = pd.DataFrame(words)\n",
    "#Количество разных слов до стемминга\n",
    "len(df1[0].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXA7Fe_izuqh"
   },
   "source": [
    "###Задание 5: Подсчитайте количество разных слов\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "BGgmHzUAzwqO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66835\n",
      "66818\n"
     ]
    }
   ],
   "source": [
    "#Количество после лемматизации и стемминга\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "dict_words_1 = {}\n",
    "for key in df_lemm[0]:\n",
    "    if stemmer.stem(key) in dict_words_1.keys():\n",
    "      dict_words_1[stemmer.stem(key)] += 1\n",
    "    else:\n",
    "      dict_words_1.update({stemmer.stem(key): 1})\n",
    "    \n",
    "df_stem_1 = pd.DataFrame(dict_words_1.items())\n",
    "print(len(df_stem_1)) \n",
    "\n",
    "#Количество после стемминга и лемматизации\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "dict_words_2 = {}\n",
    "for key in df_stem[0]:\n",
    "    if lemmatizer.lemmatize(key) in dict_words_2.keys():\n",
    "      dict_words_2[lemmatizer.lemmatize(key)] += 1\n",
    "    else:\n",
    "      dict_words_2.update({lemmatizer.lemmatize(key): 1})\n",
    "    \n",
    "df_lemm_1 = pd.DataFrame(dict_words_2.items())\n",
    "print(len(df_lemm_1)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At9iloRCVShn"
   },
   "source": [
    "REGEXP\n",
    "\n",
    "https://www.programiz.com/python-programming/regex \n",
    "\n",
    "https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "521aLisyVUg_",
    "outputId": "fd21b30e-aadf-47eb-e776-ba59ff427d7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search unsuccessful.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = 'a*s'\n",
    "test_string = 'abyss'\n",
    "result = re.match(pattern, test_string)\n",
    "\n",
    "if result:\n",
    "  print(\"Search successful.\")\n",
    "else:\n",
    "  print(\"Search unsuccessful.\")\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH7qx_irU4Y8"
   },
   "source": [
    "###Задание 6: \n",
    "https://www.hackerrank.com/challenges/matching-specific-string/problem?isFullScreen=true \n",
    "\n",
    "###Задание 7: \n",
    "https://www.hackerrank.com/challenges/matching-whitespace-non-whitespace-character/problem?isFullScreen=true\n",
    "\n",
    "###Задание 8: \n",
    "https://www.hackerrank.com/challenges/matching-start-end/problem?isFullScreen=true\n",
    "\n",
    "###Задание 9: \n",
    "https://www.hackerrank.com/challenges/matching-word-boundaries/problem?isFullScreen=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Csv2YN2IRXJB"
   },
   "source": [
    "Bag Of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8YWG3JhSFeZ",
    "outputId": "325b4fa5-7d56-4993-b4d0-1aae78bad94e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'learning']\n",
      "['learning', 'is', 'a', 'good', 'practice']\n",
      "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'is', 'a', 'good', 'practice']\n",
      "['welcome', 'great', 'learning', 'now', 'start', 'good', 'practice']\n",
      "[1, 1, 2, 1, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def vectorize(tokens):\n",
    "    ''' This function takes list of words in a sentence as input \n",
    "    and returns a vector of size of filtered_vocab.It puts 0 if the \n",
    "    word is not present in tokens and count of token if present.'''\n",
    "    vector=[]\n",
    "    for w in filtered_vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "def unique(sequence):\n",
    "    '''This functions returns a list in which the order remains \n",
    "    same and no item repeats.Using the set() function does not \n",
    "    preserve the original ordering,so i didnt use that instead'''\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "#create a list of stopwords.You can import stopwords from nltk too\n",
    "stopwords=[\"to\",\"is\",\"a\"]\n",
    "\n",
    "#list of special characters.You can use regular expressions too\n",
    "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n",
    "\n",
    "#Write the sentences in the corpus,in our case, just two \n",
    "string1=\"Welcome to Great Learning , Now start learning\"\n",
    "string2=\"Learning is a good practice\"\n",
    "\n",
    "#convert them to lower case\n",
    "string1=string1.lower()\n",
    "string2=string2.lower()\n",
    "\n",
    "#split the sentences into tokens\n",
    "tokens1=string1.split()\n",
    "tokens2=string2.split()\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "\n",
    "#create a vocabulary list\n",
    "vocab=unique(tokens1+tokens2)\n",
    "print(vocab)\n",
    "\n",
    "#filter the vocabulary list\n",
    "filtered_vocab=[]\n",
    "for w in vocab: \n",
    "    if w not in stopwords and w not in special_char: \n",
    "        filtered_vocab.append(w)\n",
    "print(filtered_vocab)\n",
    "\n",
    "#convert sentences into vectords\n",
    "vector1=vectorize(tokens1)\n",
    "print(vector1)\n",
    "vector2=vectorize(tokens2)\n",
    "print(vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOZ1qx05Q46b"
   },
   "source": [
    "Задание 10: Реализовать Bag of words на data_tok (можно на NLTK, можно без)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tew2nQN4OCiW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP1",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
